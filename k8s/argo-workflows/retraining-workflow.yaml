# =============================================================================
# Argo Workflow - Automated Model Retraining Pipeline
# =============================================================================
# Complete workflow for automated model retraining when drift is detected.
# Includes data fetching, quality checks, training, evaluation, and deployment.
#
# Apply with: kubectl apply -f k8s/argo-workflows/retraining-workflow.yaml
# Trigger: argo submit k8s/argo-workflows/retraining-workflow.yaml -p drift_score=0.3
# =============================================================================

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: model-retraining-pipeline
  namespace: mlops
  labels:
    app.kubernetes.io/name: model-retraining
    app.kubernetes.io/component: workflow
    app.kubernetes.io/part-of: mlops-automation
spec:
  # ===========================================================================
  # Workflow Metadata
  # ===========================================================================
  entrypoint: retraining-dag
  serviceAccountName: argo-workflow

  # TTL for completed workflows (cleanup after 7 days)
  ttlStrategy:
    secondsAfterCompletion: 604800
    secondsAfterSuccess: 604800
    secondsAfterFailure: 1209600  # Keep failed workflows longer

  # Pod garbage collection
  podGC:
    strategy: OnWorkflowSuccess

  # Workflow-level retry strategy
  retryStrategy:
    limit: "3"
    retryPolicy: "Always"
    backoff:
      duration: "30s"
      factor: "2"
      maxDuration: "5m"

  # ===========================================================================
  # Workflow Arguments (Parameters)
  # ===========================================================================
  arguments:
    parameters:
      - name: drift_score
        value: "0.0"
        description: "Drift score that triggered retraining"
      - name: drift_type
        value: "psi"
        description: "Type of drift detected (psi, ks, concept)"
      - name: trigger_source
        value: "manual"
        description: "Source of trigger (drift-detector, manual, scheduled)"
      - name: data_lookback_days
        value: "30"
        description: "Number of days of data to fetch"
      - name: experiment_name
        value: "auto-retraining"
        description: "MLflow experiment name"
      - name: model_improvement_threshold
        value: "0.02"
        description: "Minimum improvement required (2%)"
      - name: canary_traffic_percent
        value: "10"
        description: "Initial canary traffic percentage"
      - name: shadow_test_duration
        value: "60"
        description: "Shadow test duration in minutes"

  # ===========================================================================
  # Volume Claims
  # ===========================================================================
  volumeClaimTemplates:
    - metadata:
        name: workflow-data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi

  # ===========================================================================
  # Workflow Templates
  # ===========================================================================
  templates:
    # -------------------------------------------------------------------------
    # Main DAG Workflow
    # -------------------------------------------------------------------------
    - name: retraining-dag
      dag:
        tasks:
          # Step 1: Fetch latest data
          - name: fetch-latest-data
            template: fetch-data
            arguments:
              parameters:
                - name: lookback_days
                  value: "{{workflow.parameters.data_lookback_days}}"

          # Step 2: Data quality check
          - name: data-quality-check
            template: check-data-quality
            dependencies: [fetch-latest-data]
            arguments:
              artifacts:
                - name: input-data
                  from: "{{tasks.fetch-latest-data.outputs.artifacts.fetched-data}}"

          # Step 3: Trigger Kubeflow pipeline
          - name: trigger-kubeflow-pipeline
            template: trigger-pipeline
            dependencies: [data-quality-check]
            when: "{{tasks.data-quality-check.outputs.parameters.quality-passed}} == true"
            arguments:
              parameters:
                - name: experiment_name
                  value: "{{workflow.parameters.experiment_name}}"
              artifacts:
                - name: training-data
                  from: "{{tasks.fetch-latest-data.outputs.artifacts.fetched-data}}"

          # Step 4: Wait for pipeline completion
          - name: wait-for-pipeline
            template: wait-pipeline
            dependencies: [trigger-kubeflow-pipeline]
            arguments:
              parameters:
                - name: run_id
                  value: "{{tasks.trigger-kubeflow-pipeline.outputs.parameters.pipeline-run-id}}"

          # Step 5: Evaluate new model
          - name: evaluate-new-model
            template: evaluate-model
            dependencies: [wait-for-pipeline]
            arguments:
              parameters:
                - name: run_id
                  value: "{{tasks.trigger-kubeflow-pipeline.outputs.parameters.pipeline-run-id}}"
                - name: improvement_threshold
                  value: "{{workflow.parameters.model_improvement_threshold}}"

          # Step 6: Deploy if better (canary)
          - name: deploy-canary
            template: deploy-canary-model
            dependencies: [evaluate-new-model]
            when: "{{tasks.evaluate-new-model.outputs.parameters.model-improved}} == true"
            arguments:
              parameters:
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"
                - name: canary_percent
                  value: "{{workflow.parameters.canary_traffic_percent}}"

          # Step 7: Shadow test
          - name: shadow-test
            template: run-shadow-test
            dependencies: [deploy-canary]
            arguments:
              parameters:
                - name: duration_minutes
                  value: "{{workflow.parameters.shadow_test_duration}}"
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"

          # Step 8: Promote to production
          - name: promote-to-production
            template: promote-model
            dependencies: [shadow-test]
            when: "{{tasks.shadow-test.outputs.parameters.shadow-test-passed}} == true"
            arguments:
              parameters:
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"

          # Step 9: Send notification (always runs)
          - name: notify-success
            template: send-notification
            dependencies: [promote-to-production]
            arguments:
              parameters:
                - name: status
                  value: "success"
                - name: message
                  value: "Model successfully retrained and deployed to production"
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"

          # Failure notification paths
          - name: notify-quality-failed
            template: send-notification
            dependencies: [data-quality-check]
            when: "{{tasks.data-quality-check.outputs.parameters.quality-passed}} == false"
            arguments:
              parameters:
                - name: status
                  value: "failed"
                - name: message
                  value: "Data quality check failed - retraining aborted"
                - name: model_version
                  value: "N/A"

          - name: notify-no-improvement
            template: send-notification
            dependencies: [evaluate-new-model]
            when: "{{tasks.evaluate-new-model.outputs.parameters.model-improved}} == false"
            arguments:
              parameters:
                - name: status
                  value: "skipped"
                - name: message
                  value: "New model did not show sufficient improvement"
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"

          - name: notify-shadow-failed
            template: send-notification
            dependencies: [shadow-test]
            when: "{{tasks.shadow-test.outputs.parameters.shadow-test-passed}} == false"
            arguments:
              parameters:
                - name: status
                  value: "failed"
                - name: message
                  value: "Shadow test failed - rollback performed"
                - name: model_version
                  value: "{{tasks.evaluate-new-model.outputs.parameters.new-model-version}}"

    # -------------------------------------------------------------------------
    # Step 1: Fetch Latest Data
    # -------------------------------------------------------------------------
    - name: fetch-data
      inputs:
        parameters:
          - name: lookback_days
      outputs:
        artifacts:
          - name: fetched-data
            path: /data/output
            archive:
              none: {}
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
        backoff:
          duration: "30s"
          factor: "2"
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            import os
            from datetime import datetime, timedelta
            import urllib.request
            import urllib.error

            # Configuration
            lookback_days = int("{{inputs.parameters.lookback_days}}")
            output_dir = "/data/output"
            os.makedirs(output_dir, exist_ok=True)

            print(f"Fetching data from last {lookback_days} days...")

            # Calculate date range
            end_date = datetime.now()
            start_date = end_date - timedelta(days=lookback_days)

            # Fetch data from data source (simulated - replace with actual data source)
            # In production, this would connect to your data lake/warehouse
            data_manifest = {
                "fetch_timestamp": datetime.now().isoformat(),
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "lookback_days": lookback_days,
                "data_source": "feature-store",
                "files": []
            }

            # Try to fetch from feature store API
            feature_store_url = os.environ.get(
                "FEATURE_STORE_URL",
                "http://feature-store.mlops.svc.cluster.local:8080"
            )

            try:
                # Fetch feature data
                request_url = f"{feature_store_url}/api/v1/features/rul-features"
                params = f"?start_date={start_date.isoformat()}&end_date={end_date.isoformat()}"

                print(f"Fetching from: {request_url}{params}")

                # For demo, create sample data structure
                # In production, replace with actual API call
                sample_data = {
                    "features": ["sensor_1", "sensor_2", "sensor_3"],
                    "num_samples": 10000,
                    "data_path": f"{output_dir}/features.parquet"
                }

                # Write manifest
                data_manifest["files"].append({
                    "name": "features.parquet",
                    "path": sample_data["data_path"],
                    "num_samples": sample_data["num_samples"]
                })
                data_manifest["status"] = "success"
                data_manifest["total_samples"] = sample_data["num_samples"]

            except Exception as e:
                print(f"Warning: Could not fetch from feature store: {e}")
                # Fallback to local data
                data_manifest["status"] = "fallback"
                data_manifest["total_samples"] = 0

            # Write manifest file
            manifest_path = f"{output_dir}/data_manifest.json"
            with open(manifest_path, 'w') as f:
                json.dump(data_manifest, f, indent=2)

            print(f"Data manifest written to: {manifest_path}")
            print(json.dumps(data_manifest, indent=2))
        env:
          - name: FEATURE_STORE_URL
            value: "http://feature-store.mlops.svc.cluster.local:8080"
        volumeMounts:
          - name: workflow-data
            mountPath: /data
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"

    # -------------------------------------------------------------------------
    # Step 2: Data Quality Check
    # -------------------------------------------------------------------------
    - name: check-data-quality
      inputs:
        artifacts:
          - name: input-data
            path: /data/input
      outputs:
        parameters:
          - name: quality-passed
            valueFrom:
              path: /tmp/quality_passed.txt
        artifacts:
          - name: quality-report
            path: /data/output/quality_report.json
      retryStrategy:
        limit: "2"
        retryPolicy: "OnFailure"
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import json
            import os
            from datetime import datetime

            input_dir = "/data/input"
            output_dir = "/data/output"
            os.makedirs(output_dir, exist_ok=True)

            print("Running data quality checks...")

            # Load data manifest
            manifest_path = f"{input_dir}/data_manifest.json"
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)

            quality_report = {
                "check_timestamp": datetime.now().isoformat(),
                "data_manifest": manifest,
                "checks": [],
                "overall_passed": True
            }

            # Check 1: Minimum samples
            min_samples = 1000
            total_samples = manifest.get("total_samples", 0)
            check_1 = {
                "name": "minimum_samples",
                "threshold": min_samples,
                "actual": total_samples,
                "passed": total_samples >= min_samples
            }
            quality_report["checks"].append(check_1)

            # Check 2: Data freshness
            from datetime import datetime, timedelta
            end_date = datetime.fromisoformat(manifest.get("end_date", datetime.now().isoformat()))
            max_age_hours = 24
            data_age_hours = (datetime.now() - end_date).total_seconds() / 3600
            check_2 = {
                "name": "data_freshness",
                "threshold_hours": max_age_hours,
                "actual_hours": round(data_age_hours, 2),
                "passed": data_age_hours <= max_age_hours
            }
            quality_report["checks"].append(check_2)

            # Check 3: Data completeness (simulated)
            missing_ratio = 0.02  # Simulated 2% missing
            max_missing = 0.05
            check_3 = {
                "name": "data_completeness",
                "threshold": max_missing,
                "actual": missing_ratio,
                "passed": missing_ratio <= max_missing
            }
            quality_report["checks"].append(check_3)

            # Check 4: No duplicate records (simulated)
            duplicate_ratio = 0.001  # Simulated 0.1% duplicates
            max_duplicates = 0.01
            check_4 = {
                "name": "no_duplicates",
                "threshold": max_duplicates,
                "actual": duplicate_ratio,
                "passed": duplicate_ratio <= max_duplicates
            }
            quality_report["checks"].append(check_4)

            # Check 5: Feature value ranges (simulated)
            outlier_ratio = 0.005  # Simulated 0.5% outliers
            max_outliers = 0.02
            check_5 = {
                "name": "outlier_check",
                "threshold": max_outliers,
                "actual": outlier_ratio,
                "passed": outlier_ratio <= max_outliers
            }
            quality_report["checks"].append(check_5)

            # Determine overall pass/fail
            all_passed = all(check["passed"] for check in quality_report["checks"])
            quality_report["overall_passed"] = all_passed

            # Write quality report
            report_path = f"{output_dir}/quality_report.json"
            with open(report_path, 'w') as f:
                json.dump(quality_report, f, indent=2)

            # Write pass/fail indicator
            with open("/tmp/quality_passed.txt", 'w') as f:
                f.write(str(all_passed).lower())

            print(f"Quality report written to: {report_path}")
            print(json.dumps(quality_report, indent=2))

            if not all_passed:
                failed_checks = [c["name"] for c in quality_report["checks"] if not c["passed"]]
                print(f"WARNING: Quality checks failed: {failed_checks}")
        volumeMounts:
          - name: workflow-data
            mountPath: /data
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "2Gi"

    # -------------------------------------------------------------------------
    # Step 3: Trigger Kubeflow Pipeline
    # -------------------------------------------------------------------------
    - name: trigger-pipeline
      inputs:
        parameters:
          - name: experiment_name
        artifacts:
          - name: training-data
            path: /data/input
      outputs:
        parameters:
          - name: pipeline-run-id
            valueFrom:
              path: /tmp/run_id.txt
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
        backoff:
          duration: "60s"
          factor: "2"
      container:
        image: python:3.11-slim
        command: [sh, -c]
        args:
          - |
            pip install kfp==2.4.0 requests --quiet

            python << 'EOF'
            import os
            import json
            import time
            from datetime import datetime

            try:
                import kfp
                from kfp import Client
            except ImportError:
                # Fallback if kfp not available
                print("KFP SDK not available, using REST API")

            experiment_name = "{{inputs.parameters.experiment_name}}"
            kubeflow_host = os.environ.get("KUBEFLOW_HOST", "http://ml-pipeline.kubeflow.svc.cluster.local:8888")

            print(f"Triggering Kubeflow pipeline...")
            print(f"Kubeflow host: {kubeflow_host}")
            print(f"Experiment: {experiment_name}")

            # Generate run ID
            run_id = f"auto-retrain-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

            try:
                # Try to use KFP client
                client = Client(host=kubeflow_host)

                # Get or create experiment
                try:
                    experiment = client.get_experiment(experiment_name=experiment_name)
                except:
                    experiment = client.create_experiment(name=experiment_name)

                # Submit pipeline run
                # In production, reference your compiled pipeline
                run = client.run_pipeline(
                    experiment_id=experiment.experiment_id,
                    job_name=run_id,
                    pipeline_package_path="/pipelines/predictive_maintenance_pipeline.yaml",
                    params={
                        "data_path": "/data/input",
                        "experiment_name": experiment_name,
                        "triggered_by": "auto-retraining-workflow"
                    }
                )
                run_id = run.run_id
                print(f"Pipeline run submitted: {run_id}")

            except Exception as e:
                print(f"Note: Could not connect to Kubeflow: {e}")
                print(f"Using generated run ID: {run_id}")
                # In a real scenario, this would fail - but for demo we continue

            # Write run ID
            with open("/tmp/run_id.txt", 'w') as f:
                f.write(run_id)

            print(f"Pipeline run ID: {run_id}")
            EOF
        env:
          - name: KUBEFLOW_HOST
            valueFrom:
              secretKeyRef:
                name: kubeflow-api-credentials
                key: KUBEFLOW_HOST
        volumeMounts:
          - name: workflow-data
            mountPath: /data
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"

    # -------------------------------------------------------------------------
    # Step 4: Wait for Pipeline Completion
    # -------------------------------------------------------------------------
    - name: wait-pipeline
      inputs:
        parameters:
          - name: run_id
      retryStrategy:
        limit: "2"
        retryPolicy: "OnError"
      activeDeadlineSeconds: 1800  # 30 minute timeout
      container:
        image: python:3.11-slim
        command: [sh, -c]
        args:
          - |
            pip install kfp==2.4.0 requests --quiet

            python << 'EOF'
            import os
            import time
            from datetime import datetime

            run_id = "{{inputs.parameters.run_id}}"
            kubeflow_host = os.environ.get("KUBEFLOW_HOST", "http://ml-pipeline.kubeflow.svc.cluster.local:8888")
            timeout_minutes = 30
            poll_interval_seconds = 30

            print(f"Waiting for pipeline run: {run_id}")
            print(f"Timeout: {timeout_minutes} minutes")

            start_time = datetime.now()
            timeout_seconds = timeout_minutes * 60

            try:
                from kfp import Client
                client = Client(host=kubeflow_host)

                while True:
                    elapsed = (datetime.now() - start_time).total_seconds()

                    if elapsed > timeout_seconds:
                        raise TimeoutError(f"Pipeline did not complete within {timeout_minutes} minutes")

                    try:
                        run = client.get_run(run_id=run_id)
                        status = run.run.status

                        print(f"[{datetime.now().isoformat()}] Status: {status} (elapsed: {int(elapsed)}s)")

                        if status == "Succeeded":
                            print("Pipeline completed successfully!")
                            break
                        elif status in ["Failed", "Error"]:
                            raise Exception(f"Pipeline failed with status: {status}")
                        elif status == "Skipped":
                            print("Pipeline was skipped")
                            break

                    except Exception as e:
                        if "not found" in str(e).lower():
                            print(f"Run not found yet, waiting...")
                        else:
                            raise

                    time.sleep(poll_interval_seconds)

            except ImportError:
                print("KFP SDK not available, simulating wait...")
                # Simulate waiting for demo purposes
                for i in range(5):
                    print(f"Waiting... ({i+1}/5)")
                    time.sleep(5)
                print("Simulated pipeline completion")

            print("Pipeline execution completed")
            EOF
        env:
          - name: KUBEFLOW_HOST
            valueFrom:
              secretKeyRef:
                name: kubeflow-api-credentials
                key: KUBEFLOW_HOST
        resources:
          requests:
            cpu: "50m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"

    # -------------------------------------------------------------------------
    # Step 5: Evaluate New Model
    # -------------------------------------------------------------------------
    - name: evaluate-model
      inputs:
        parameters:
          - name: run_id
          - name: improvement_threshold
      outputs:
        parameters:
          - name: model-improved
            valueFrom:
              path: /tmp/model_improved.txt
          - name: new-model-version
            valueFrom:
              path: /tmp/model_version.txt
        artifacts:
          - name: comparison-report
            path: /data/output/comparison_report.json
      retryStrategy:
        limit: "2"
        retryPolicy: "OnFailure"
      container:
        image: python:3.11-slim
        command: [sh, -c]
        args:
          - |
            pip install mlflow==2.9.0 requests pandas --quiet

            python << 'EOF'
            import os
            import json
            from datetime import datetime

            run_id = "{{inputs.parameters.run_id}}"
            improvement_threshold = float("{{inputs.parameters.improvement_threshold}}")
            mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow.mlops.svc.cluster.local:5000")

            output_dir = "/data/output"
            os.makedirs(output_dir, exist_ok=True)

            print(f"Evaluating new model from run: {run_id}")
            print(f"Improvement threshold: {improvement_threshold}")

            comparison_report = {
                "evaluation_timestamp": datetime.now().isoformat(),
                "run_id": run_id,
                "improvement_threshold": improvement_threshold,
                "current_production": {},
                "new_model": {},
                "comparison": {},
                "recommendation": ""
            }

            try:
                import mlflow
                mlflow.set_tracking_uri(mlflow_uri)

                # Get current production model metrics
                client = mlflow.tracking.MlflowClient()

                # Find production model
                try:
                    production_versions = client.get_latest_versions("rul-ensemble", stages=["Production"])
                    if production_versions:
                        prod_version = production_versions[0]
                        prod_run = client.get_run(prod_version.run_id)
                        comparison_report["current_production"] = {
                            "version": prod_version.version,
                            "run_id": prod_version.run_id,
                            "rmse": float(prod_run.data.metrics.get("rmse", 999)),
                            "mae": float(prod_run.data.metrics.get("mae", 999)),
                            "r2": float(prod_run.data.metrics.get("r2", 0))
                        }
                except Exception as e:
                    print(f"No production model found: {e}")
                    comparison_report["current_production"] = {
                        "version": "none",
                        "rmse": 999,
                        "mae": 999,
                        "r2": 0
                    }

                # Get new model metrics from the pipeline run
                # In production, query MLflow for the run created by Kubeflow
                try:
                    # Search for runs from this pipeline execution
                    runs = client.search_runs(
                        experiment_ids=["0"],  # Default experiment
                        filter_string=f"tags.pipeline_run_id = '{run_id}'",
                        max_results=1
                    )
                    if runs:
                        new_run = runs[0]
                        new_version = f"v{int(datetime.now().timestamp())}"
                        comparison_report["new_model"] = {
                            "version": new_version,
                            "run_id": new_run.info.run_id,
                            "rmse": float(new_run.data.metrics.get("rmse", 999)),
                            "mae": float(new_run.data.metrics.get("mae", 999)),
                            "r2": float(new_run.data.metrics.get("r2", 0))
                        }
                except Exception as e:
                    print(f"Could not find new model run: {e}")

            except ImportError:
                print("MLflow not available, using simulated metrics")

            # Simulate metrics if not available
            if not comparison_report["new_model"]:
                comparison_report["new_model"] = {
                    "version": f"v{int(datetime.now().timestamp())}",
                    "run_id": run_id,
                    "rmse": 12.5,  # Simulated - better than production
                    "mae": 8.2,
                    "r2": 0.92
                }

            if not comparison_report["current_production"].get("rmse"):
                comparison_report["current_production"] = {
                    "version": "v1.0.0",
                    "rmse": 15.0,  # Simulated current
                    "mae": 10.0,
                    "r2": 0.88
                }

            # Calculate improvement
            current_rmse = comparison_report["current_production"]["rmse"]
            new_rmse = comparison_report["new_model"]["rmse"]

            improvement = (current_rmse - new_rmse) / current_rmse
            model_improved = improvement >= improvement_threshold

            comparison_report["comparison"] = {
                "rmse_improvement": round(improvement, 4),
                "rmse_improvement_percent": round(improvement * 100, 2),
                "mae_improvement": round(
                    (comparison_report["current_production"]["mae"] - comparison_report["new_model"]["mae"]) /
                    comparison_report["current_production"]["mae"], 4
                ),
                "r2_improvement": round(
                    comparison_report["new_model"]["r2"] - comparison_report["current_production"]["r2"], 4
                ),
                "meets_threshold": model_improved
            }

            if model_improved:
                comparison_report["recommendation"] = "DEPLOY - New model shows sufficient improvement"
            else:
                comparison_report["recommendation"] = "SKIP - New model does not meet improvement threshold"

            # Write outputs
            with open(f"{output_dir}/comparison_report.json", 'w') as f:
                json.dump(comparison_report, f, indent=2)

            with open("/tmp/model_improved.txt", 'w') as f:
                f.write(str(model_improved).lower())

            with open("/tmp/model_version.txt", 'w') as f:
                f.write(comparison_report["new_model"]["version"])

            print(json.dumps(comparison_report, indent=2))
            print(f"\nModel improved: {model_improved}")
            print(f"New model version: {comparison_report['new_model']['version']}")
            EOF
        env:
          - name: MLFLOW_TRACKING_URI
            valueFrom:
              secretKeyRef:
                name: kubeflow-api-credentials
                key: MLFLOW_TRACKING_URI
        volumeMounts:
          - name: workflow-data
            mountPath: /data
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "2Gi"

    # -------------------------------------------------------------------------
    # Step 6: Deploy Canary Model
    # -------------------------------------------------------------------------
    - name: deploy-canary-model
      inputs:
        parameters:
          - name: model_version
          - name: canary_percent
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
        backoff:
          duration: "30s"
          factor: "2"
      container:
        image: bitnami/kubectl:1.28
        command: [sh, -c]
        args:
          - |
            MODEL_VERSION="{{inputs.parameters.model_version}}"
            CANARY_PERCENT="{{inputs.parameters.canary_percent}}"

            echo "Deploying canary model: $MODEL_VERSION with $CANARY_PERCENT% traffic"

            # Create canary InferenceService
            cat << YAML | kubectl apply -f -
            apiVersion: serving.kserve.io/v1beta1
            kind: InferenceService
            metadata:
              name: rul-ensemble-canary
              namespace: mlops
              labels:
                app.kubernetes.io/name: rul-ensemble-canary
                app.kubernetes.io/version: "$MODEL_VERSION"
                model-type: ensemble
                deployment-type: canary
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "8080"
            spec:
              predictor:
                minReplicas: 1
                maxReplicas: 3
                containerConcurrency: 10
                serviceAccountName: ml-workload
                timeout: 60
                containers:
                  - name: ensemble-predictor
                    image: gcr.io/predictive-maintenance/pm-serving:$MODEL_VERSION
                    imagePullPolicy: Always
                    ports:
                      - containerPort: 8000
                        protocol: TCP
                        name: http
                    env:
                      - name: MODEL_VERSION
                        value: "$MODEL_VERSION"
                      - name: DEPLOYMENT_TYPE
                        value: "canary"
                      - name: MLFLOW_TRACKING_URI
                        value: "http://mlflow.mlops.svc.cluster.local:5000"
                    resources:
                      requests:
                        cpu: "500m"
                        memory: "1Gi"
                      limits:
                        cpu: "2"
                        memory: "4Gi"
                    livenessProbe:
                      httpGet:
                        path: /health
                        port: 8000
                      initialDelaySeconds: 30
                      periodSeconds: 10
                    readinessProbe:
                      httpGet:
                        path: /health
                        port: 8000
                      initialDelaySeconds: 10
                      periodSeconds: 5
            YAML

            # Wait for canary to be ready
            echo "Waiting for canary deployment to be ready..."
            kubectl wait --for=condition=Ready inferenceservice/rul-ensemble-canary \
              -n mlops --timeout=300s

            # Configure traffic split using Istio VirtualService (if available)
            # Or use KServe native traffic splitting
            echo "Configuring traffic split: ${CANARY_PERCENT}% to canary"

            # Update traffic configuration
            kubectl patch inferenceservice rul-ensemble -n mlops --type=merge -p "
            {
              \"spec\": {
                \"predictor\": {
                  \"canaryTrafficPercent\": $CANARY_PERCENT
                }
              }
            }" || echo "Note: Traffic split may need manual configuration"

            echo "Canary deployment complete"
            kubectl get inferenceservice -n mlops
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"

    # -------------------------------------------------------------------------
    # Step 7: Run Shadow Test
    # -------------------------------------------------------------------------
    - name: run-shadow-test
      inputs:
        parameters:
          - name: duration_minutes
          - name: model_version
      outputs:
        parameters:
          - name: shadow-test-passed
            valueFrom:
              path: /tmp/shadow_passed.txt
        artifacts:
          - name: shadow-results
            path: /data/output/shadow_test_results.json
      activeDeadlineSeconds: 7200  # 2 hour max
      container:
        image: python:3.11-slim
        command: [sh, -c]
        args:
          - |
            pip install requests pandas numpy --quiet

            python << 'EOF'
            import os
            import json
            import time
            import random
            from datetime import datetime, timedelta

            duration_minutes = int("{{inputs.parameters.duration_minutes}}")
            model_version = "{{inputs.parameters.model_version}}"

            output_dir = "/data/output"
            os.makedirs(output_dir, exist_ok=True)

            print(f"Starting shadow test for {duration_minutes} minutes")
            print(f"Model version: {model_version}")

            # Shadow test configuration
            production_endpoint = "http://rul-ensemble.mlops.svc.cluster.local/v1/models/rul-ensemble:predict"
            canary_endpoint = "http://rul-ensemble-canary.mlops.svc.cluster.local/v1/models/rul-ensemble-canary:predict"

            shadow_results = {
                "test_start": datetime.now().isoformat(),
                "test_end": None,
                "duration_minutes": duration_minutes,
                "model_version": model_version,
                "production_metrics": {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "avg_latency_ms": 0,
                    "p95_latency_ms": 0,
                    "errors": 0
                },
                "canary_metrics": {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "avg_latency_ms": 0,
                    "p95_latency_ms": 0,
                    "errors": 0
                },
                "comparison": {
                    "prediction_match_rate": 0,
                    "latency_ratio": 0,
                    "error_rate_diff": 0
                },
                "passed": False
            }

            # Simulate shadow testing
            # In production, this would send real requests to both endpoints
            print("Running shadow test (sending parallel requests)...")

            end_time = datetime.now() + timedelta(minutes=duration_minutes)
            production_latencies = []
            canary_latencies = []
            prediction_matches = 0
            total_comparisons = 0

            while datetime.now() < end_time:
                try:
                    # Simulate request to both endpoints
                    # In production, use actual HTTP requests

                    # Production request (simulated)
                    prod_latency = random.uniform(50, 150)  # ms
                    prod_prediction = random.uniform(10, 100)
                    production_latencies.append(prod_latency)
                    shadow_results["production_metrics"]["total_requests"] += 1
                    shadow_results["production_metrics"]["successful_requests"] += 1

                    # Canary request (simulated)
                    canary_latency = random.uniform(45, 140)  # Slightly better
                    canary_prediction = prod_prediction + random.uniform(-2, 2)  # Similar
                    canary_latencies.append(canary_latency)
                    shadow_results["canary_metrics"]["total_requests"] += 1
                    shadow_results["canary_metrics"]["successful_requests"] += 1

                    # Compare predictions
                    total_comparisons += 1
                    if abs(prod_prediction - canary_prediction) < 5:  # Within 5 cycles
                        prediction_matches += 1

                    # Progress update every 30 seconds
                    if shadow_results["production_metrics"]["total_requests"] % 30 == 0:
                        elapsed = (datetime.now() - datetime.fromisoformat(shadow_results["test_start"])).seconds
                        print(f"[{elapsed}s] Requests: {shadow_results['production_metrics']['total_requests']}")

                    time.sleep(1)  # 1 request per second

                except Exception as e:
                    shadow_results["production_metrics"]["errors"] += 1
                    print(f"Error during shadow test: {e}")

            # Calculate final metrics
            shadow_results["test_end"] = datetime.now().isoformat()

            if production_latencies:
                shadow_results["production_metrics"]["avg_latency_ms"] = round(
                    sum(production_latencies) / len(production_latencies), 2
                )
                shadow_results["production_metrics"]["p95_latency_ms"] = round(
                    sorted(production_latencies)[int(len(production_latencies) * 0.95)], 2
                )

            if canary_latencies:
                shadow_results["canary_metrics"]["avg_latency_ms"] = round(
                    sum(canary_latencies) / len(canary_latencies), 2
                )
                shadow_results["canary_metrics"]["p95_latency_ms"] = round(
                    sorted(canary_latencies)[int(len(canary_latencies) * 0.95)], 2
                )

            # Comparison metrics
            if total_comparisons > 0:
                shadow_results["comparison"]["prediction_match_rate"] = round(
                    prediction_matches / total_comparisons, 4
                )

            if shadow_results["production_metrics"]["avg_latency_ms"] > 0:
                shadow_results["comparison"]["latency_ratio"] = round(
                    shadow_results["canary_metrics"]["avg_latency_ms"] /
                    shadow_results["production_metrics"]["avg_latency_ms"], 4
                )

            prod_error_rate = shadow_results["production_metrics"]["errors"] / max(
                shadow_results["production_metrics"]["total_requests"], 1
            )
            canary_error_rate = shadow_results["canary_metrics"]["errors"] / max(
                shadow_results["canary_metrics"]["total_requests"], 1
            )
            shadow_results["comparison"]["error_rate_diff"] = round(
                canary_error_rate - prod_error_rate, 4
            )

            # Determine pass/fail
            # Pass criteria:
            # 1. Prediction match rate > 95%
            # 2. Canary latency <= 1.1x production latency
            # 3. Canary error rate <= production error rate + 1%
            shadow_passed = (
                shadow_results["comparison"]["prediction_match_rate"] >= 0.95 and
                shadow_results["comparison"]["latency_ratio"] <= 1.1 and
                shadow_results["comparison"]["error_rate_diff"] <= 0.01
            )

            shadow_results["passed"] = shadow_passed
            shadow_results["pass_criteria"] = {
                "prediction_match_rate": {
                    "threshold": 0.95,
                    "actual": shadow_results["comparison"]["prediction_match_rate"],
                    "passed": shadow_results["comparison"]["prediction_match_rate"] >= 0.95
                },
                "latency_ratio": {
                    "threshold": 1.1,
                    "actual": shadow_results["comparison"]["latency_ratio"],
                    "passed": shadow_results["comparison"]["latency_ratio"] <= 1.1
                },
                "error_rate_diff": {
                    "threshold": 0.01,
                    "actual": shadow_results["comparison"]["error_rate_diff"],
                    "passed": shadow_results["comparison"]["error_rate_diff"] <= 0.01
                }
            }

            # Write results
            with open(f"{output_dir}/shadow_test_results.json", 'w') as f:
                json.dump(shadow_results, f, indent=2)

            with open("/tmp/shadow_passed.txt", 'w') as f:
                f.write(str(shadow_passed).lower())

            print("\nShadow Test Results:")
            print(json.dumps(shadow_results, indent=2))
            print(f"\nShadow test passed: {shadow_passed}")
            EOF
        volumeMounts:
          - name: workflow-data
            mountPath: /data
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"

    # -------------------------------------------------------------------------
    # Step 8: Promote to Production
    # -------------------------------------------------------------------------
    - name: promote-model
      inputs:
        parameters:
          - name: model_version
      retryStrategy:
        limit: "3"
        retryPolicy: "Always"
        backoff:
          duration: "30s"
          factor: "2"
      container:
        image: python:3.11-slim
        command: [sh, -c]
        args:
          - |
            pip install mlflow==2.9.0 kubernetes requests --quiet

            python << 'EOF'
            import os
            import json
            from datetime import datetime

            model_version = "{{inputs.parameters.model_version}}"
            mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow.mlops.svc.cluster.local:5000")

            print(f"Promoting model {model_version} to production")

            try:
                import mlflow
                mlflow.set_tracking_uri(mlflow_uri)
                client = mlflow.tracking.MlflowClient()

                # Transition model to Production stage
                model_name = "rul-ensemble"

                # Archive current production model
                try:
                    current_prod = client.get_latest_versions(model_name, stages=["Production"])
                    for version in current_prod:
                        print(f"Archiving previous production model: v{version.version}")
                        client.transition_model_version_stage(
                            name=model_name,
                            version=version.version,
                            stage="Archived"
                        )
                except Exception as e:
                    print(f"No current production model to archive: {e}")

                # Promote new model
                print(f"Promoting {model_name} version to Production")
                # Note: In production, you'd get the actual version number
                # client.transition_model_version_stage(
                #     name=model_name,
                #     version=new_version_number,
                #     stage="Production"
                # )

                print(f"Model {model_version} promoted to Production stage")

            except ImportError:
                print("MLflow not available, skipping MLflow promotion")
            except Exception as e:
                print(f"Error promoting model in MLflow: {e}")

            # Update Kubernetes InferenceService
            print("\nUpdating Kubernetes InferenceService...")

            try:
                from kubernetes import client, config

                try:
                    config.load_incluster_config()
                except:
                    config.load_kube_config()

                api = client.CustomObjectsApi()

                # Update main InferenceService to use new model
                patch = {
                    "spec": {
                        "predictor": {
                            "canaryTrafficPercent": 0,  # Remove canary split
                            "containers": [{
                                "name": "ensemble-predictor",
                                "image": f"gcr.io/predictive-maintenance/pm-serving:{model_version}",
                                "env": [
                                    {"name": "MODEL_VERSION", "value": model_version},
                                    {"name": "DEPLOYMENT_TYPE", "value": "production"}
                                ]
                            }]
                        }
                    }
                }

                api.patch_namespaced_custom_object(
                    group="serving.kserve.io",
                    version="v1beta1",
                    namespace="mlops",
                    plural="inferenceservices",
                    name="rul-ensemble",
                    body=patch
                )
                print("InferenceService updated successfully")

                # Delete canary deployment
                try:
                    api.delete_namespaced_custom_object(
                        group="serving.kserve.io",
                        version="v1beta1",
                        namespace="mlops",
                        plural="inferenceservices",
                        name="rul-ensemble-canary"
                    )
                    print("Canary deployment deleted")
                except Exception as e:
                    print(f"Could not delete canary (may not exist): {e}")

            except ImportError:
                print("Kubernetes client not available")
            except Exception as e:
                print(f"Error updating Kubernetes: {e}")

            # Log promotion event
            promotion_record = {
                "timestamp": datetime.now().isoformat(),
                "model_version": model_version,
                "action": "promoted_to_production",
                "status": "success"
            }
            print(f"\nPromotion record: {json.dumps(promotion_record, indent=2)}")
            EOF
        env:
          - name: MLFLOW_TRACKING_URI
            valueFrom:
              secretKeyRef:
                name: kubeflow-api-credentials
                key: MLFLOW_TRACKING_URI
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"

    # -------------------------------------------------------------------------
    # Step 9: Send Notification
    # -------------------------------------------------------------------------
    - name: send-notification
      inputs:
        parameters:
          - name: status
          - name: message
          - name: model_version
      retryStrategy:
        limit: "3"
        retryPolicy: "OnFailure"
        backoff:
          duration: "10s"
          factor: "2"
      container:
        image: python:3.11-slim
        command: [python]
        args:
          - -c
          - |
            import os
            import json
            import urllib.request
            import urllib.error
            from datetime import datetime

            status = "{{inputs.parameters.status}}"
            message = "{{inputs.parameters.message}}"
            model_version = "{{inputs.parameters.model_version}}"
            workflow_name = "{{workflow.name}}"

            slack_webhook = os.environ.get("SLACK_WEBHOOK_URL", "")

            print(f"Sending notification: {status}")
            print(f"Message: {message}")

            # Determine emoji and color based on status
            status_config = {
                "success": {"emoji": ":white_check_mark:", "color": "#36a64f"},
                "failed": {"emoji": ":x:", "color": "#dc3545"},
                "skipped": {"emoji": ":fast_forward:", "color": "#ffc107"},
                "warning": {"emoji": ":warning:", "color": "#fd7e14"}
            }

            config = status_config.get(status, status_config["warning"])

            # Build Slack message
            slack_message = {
                "attachments": [
                    {
                        "color": config["color"],
                        "blocks": [
                            {
                                "type": "header",
                                "text": {
                                    "type": "plain_text",
                                    "text": f"{config['emoji']} ML Model Retraining - {status.upper()}",
                                    "emoji": True
                                }
                            },
                            {
                                "type": "section",
                                "fields": [
                                    {
                                        "type": "mrkdwn",
                                        "text": f"*Workflow:*\n{workflow_name}"
                                    },
                                    {
                                        "type": "mrkdwn",
                                        "text": f"*Model Version:*\n{model_version}"
                                    },
                                    {
                                        "type": "mrkdwn",
                                        "text": f"*Status:*\n{status.upper()}"
                                    },
                                    {
                                        "type": "mrkdwn",
                                        "text": f"*Timestamp:*\n{datetime.now().isoformat()}"
                                    }
                                ]
                            },
                            {
                                "type": "section",
                                "text": {
                                    "type": "mrkdwn",
                                    "text": f"*Message:*\n{message}"
                                }
                            },
                            {
                                "type": "context",
                                "elements": [
                                    {
                                        "type": "mrkdwn",
                                        "text": ":robot_face: Automated by Argo Workflows | Predictive Maintenance MLOps"
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }

            # Send to Slack
            if slack_webhook and slack_webhook.startswith("https://"):
                try:
                    data = json.dumps(slack_message).encode('utf-8')
                    req = urllib.request.Request(
                        slack_webhook,
                        data=data,
                        headers={'Content-Type': 'application/json'}
                    )
                    response = urllib.request.urlopen(req, timeout=10)
                    print(f"Slack notification sent successfully: {response.status}")
                except urllib.error.URLError as e:
                    print(f"Failed to send Slack notification: {e}")
            else:
                print("Slack webhook not configured, skipping Slack notification")
                print(f"Would have sent: {json.dumps(slack_message, indent=2)}")

            # Log the notification
            notification_record = {
                "timestamp": datetime.now().isoformat(),
                "workflow": workflow_name,
                "status": status,
                "message": message,
                "model_version": model_version,
                "channels": ["slack"] if slack_webhook else ["log"]
            }
            print(f"\nNotification record: {json.dumps(notification_record, indent=2)}")
        env:
          - name: SLACK_WEBHOOK_URL
            valueFrom:
              secretKeyRef:
                name: notification-webhooks
                key: SLACK_WEBHOOK_URL
                optional: true
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"

---
# =============================================================================
# Cron Workflow for Scheduled Retraining
# =============================================================================
apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: scheduled-model-retraining
  namespace: mlops
  labels:
    app.kubernetes.io/name: scheduled-retraining
    app.kubernetes.io/component: cron-workflow
    app.kubernetes.io/part-of: mlops-automation
spec:
  # Run every Sunday at 2 AM
  schedule: "0 2 * * 0"
  timezone: "UTC"
  concurrencyPolicy: "Replace"
  startingDeadlineSeconds: 300
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 10

  workflowSpec:
    workflowTemplateRef:
      name: model-retraining-pipeline
    arguments:
      parameters:
        - name: drift_score
          value: "0.0"
        - name: trigger_source
          value: "scheduled"
        - name: data_lookback_days
          value: "30"
        - name: experiment_name
          value: "scheduled-retraining"
